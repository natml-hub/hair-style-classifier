{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hair Style Classification\n",
    "Zero-shot hair style classification from OpenAI CLIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /private/var/folders/98/2x4_h0694k17yxw6khx3tmx80000gn/T/pip-req-build-1za9sy7m\n",
      "  Running command git clone -q https://github.com/openai/CLIP.git /private/var/folders/98/2x4_h0694k17yxw6khx3tmx80000gn/T/pip-req-build-1za9sy7m\n",
      "  Resolved https://github.com/openai/CLIP.git to commit 3702849800aa56e2223035bccd1c6ef91c704ca8\n",
      "Requirement already satisfied: ftfy in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from clip==1.0) (6.1.1)\n",
      "Requirement already satisfied: regex in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from clip==1.0) (2022.9.13)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from clip==1.0) (4.64.1)\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from clip==1.0) (1.12.1)\n",
      "Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from clip==1.0) (0.13.1)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /Users/yusuf/Library/Python/3.9/lib/python/site-packages (from ftfy->clip==1.0) (0.2.5)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from torch->clip==1.0) (4.3.0)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from torchvision->clip==1.0) (2.28.1)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from torchvision->clip==1.0) (1.23.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from torchvision->clip==1.0) (9.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->torchvision->clip==1.0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->torchvision->clip==1.0) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->torchvision->clip==1.0) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->torchvision->clip==1.0) (2022.12.7)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.0 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install CLIP\n",
    "%pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from tempfile import gettempdir\n",
    "from torch import no_grad\n",
    "import clip\n",
    "\n",
    "def predict (image: Image.Image) -> str:\n",
    "    \"\"\"\n",
    "    Predict whether hair in a given image is curly or straight.\n",
    "\n",
    "    Parameters:\n",
    "        image (PIL.Image): Input image.\n",
    "\n",
    "    Returns:\n",
    "        str: One of [\"CURLY\", \"STRAIGHT\"].\n",
    "    \"\"\"\n",
    "    # Load CLIP\n",
    "    # Use `download_root=gettempdir()` cos we're creating an endpoint with `SERVERLESS` acceleration\n",
    "    # Serverless endpoints don't have writable file systems except for the temp directory\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=\"cpu\", download_root=gettempdir())\n",
    "    image_feature = preprocess(image).unsqueeze(dim=0)\n",
    "    # Infer\n",
    "    PROMPTS = {\n",
    "        \"CURLY\": \"curly hair, twisted hair, loc hair, bundled hair, tangled hair\",\n",
    "        \"STRAIGHT\": \"straight hair, pressed hair, flat hair\"\n",
    "    }\n",
    "    with no_grad():\n",
    "        prompt = clip.tokenize(list(PROMPTS.values()))\n",
    "        _, logits_per_text = model(image_feature, prompt)\n",
    "    # Get label\n",
    "    result_idx = logits_per_text.argmax()\n",
    "    result = list(PROMPTS.keys())[result_idx]\n",
    "    # Return\n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
